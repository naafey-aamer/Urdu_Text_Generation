{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Modelling and Text Generation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "An n-gram is a contiguous sequence of n words. For example \"Machine\" is a unigram, \"Machine Learning\" is a bigram and \"Machine Learning PA2\" is a trigram. In language modeling, n-gram models are probabilistic models of text that use word dependencies and context to predict the likelihood of occurence of an n-gram, i.e. predicting the nth word in an n-gram based on the previous n-1 words:\n",
    "$$\n",
    "P(ngram) =  P(word|context) = P(x^{n}|x^{n-1},...,x^{1})\n",
    "$$\n",
    "One use of the predictions made by such a model is text generation. In this part you will be training your own n-gram model and using it to generate text after learning from the provided Urdu short stories. \n",
    "<br><br>\n",
    "For additional details of the working of n-gram models, you can also consult [Chapter 3](https://web.stanford.edu/~jurafsky/slp3/3.pdf) of the Speech and Language Processing book as and references.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "You will be using the Urdu short stories by Patras Bukhari given in the folder `Urdu Short Stories`. This contains 6 stories of varying lengths which will serve as inputs for your n-gram model. \n",
    "We will implement an n-gram model that uses the given stories to generate Urdu text that mimics the input stories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing all required libraries here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required libraries here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import scipy as sp\n",
    "import nltk\n",
    "import os\n",
    "import urduhack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Loading and Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the short story files given and tokenize the text to be preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "data=[]\n",
    "os.chdir(r\"C:\\Users\\Lenovo\\Desktop\\Assignment 4\\DataP1\")\n",
    "for file in os.listdir():\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(file, \"r\", encoding='utf8') as f:\n",
    "            data.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the tokenized data. Go through the data and use your own discretion to decide on what kind of pre-processing might be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "tokenized=[]\n",
    "for i in range(6):\n",
    "     tokenized.append(urduhack.preprocessing.remove_punctuation(data[i]))\n",
    "tokenized = [urduhack.preprocessing.remove_accents(i) for i in tokenized]\n",
    "#tokenized=[nltk.word_tokenize(i) for i in tokenized]\n",
    "for i in range(len(tokenized)):\n",
    "    tokenized[i] = tokenized[i].replace(\"\\n\", \"\")\n",
    "y=[]\n",
    "for i in range(6):\n",
    "    y.append(tokenized[i].split(\" \"))\n",
    "stop_words=urduhack.stop_words.STOP_WORDS\n",
    "y=[[w for w in i if not w in stop_words] for i in y]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Creating Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by training a unigram model. For a unigram model, the n-gram probability is approximated by probability of the word in the unigram, as the model assumes independence:\n",
    "\n",
    "$$\n",
    "P(word) = \\frac{n}{N}\n",
    "$$\n",
    "\n",
    "where n = count of the word in the corpus and N = total number of words in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a list of unigrams. Print the first 10 unigrams obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram: \n",
      "سینما\n",
      "عشق\n",
      "عنوان\n",
      "عجب\n",
      "ہوس\n",
      "خیز\n",
      "افسوس\n",
      "مضمون\n",
      "توقعات\n",
      "مجروح\n"
     ]
    }
   ],
   "source": [
    "# code here\n",
    "unigram = y\n",
    "unigram = [i for list in unigram for i in list]\n",
    "print(\"Unigram: \")\n",
    "unigram_set = set(unigram)\n",
    "unigram_set = list(unigram)\n",
    "for i in range(10):\n",
    "    print(unigram_set[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the probabilities for each unique unigram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3080\n"
     ]
    }
   ],
   "source": [
    "# code here\n",
    "uni_dict={}\n",
    "for i in unigram_set:\n",
    "    uni_dict[i]=(unigram.count(i))/len(unigram)\n",
    "print(len(list(uni_dict.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Creating Bigrams\n",
    "Now train a bigram model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a list of bigrams. Print the first 10 bigrams obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "منصوبے باندھ\n",
      "انگلیوں بربط\n",
      "طول البلد\n",
      "سورج کرنیں\n",
      "پیداوار طلباء\n",
      "کتابوں متعلق\n",
      "لاہور حدود\n",
      "بہنے شغل\n",
      "باتیں کیں\n",
      "دس لالہ\n"
     ]
    }
   ],
   "source": [
    "# code here\n",
    "bigram=[]\n",
    "for i in range(len(unigram)):\n",
    "    if i+1<len(unigram):\n",
    "        bigram.append(unigram[i]+\" \"+unigram[i+1])\n",
    "bigram_set = set(bigram)\n",
    "bigram_set = list(bigram_set)\n",
    "for i in range(10):\n",
    "    print(bigram_set[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the probabilities for each unique bigram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000001017\n"
     ]
    }
   ],
   "source": [
    "# code here\n",
    "bi_dict={}\n",
    "for i in bigram_set:\n",
    "    bi_dict[i]=(bigram.count(i))/len(bigram)\n",
    "prob = list(bi_dict.values())\n",
    "print(sum(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Creating Trigrams\n",
    "Lastly train a trigram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a list of trigrams. Print the first 10 trigrams obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مسافروں سفر نقل\n",
      "داخل اندھیرا گھپ\n",
      "بھیجنے شہر روایات\n",
      "سینے دردمند دل\n",
      "دکھتی رگ نام\n",
      "بنائیں چنانچہ تجویز\n",
      "ليے یوں قوت\n",
      "آئندہ لوگ پروفیسر\n",
      "فارسی شخص فیل\n",
      "تقریبا غلط حقیقت\n"
     ]
    }
   ],
   "source": [
    "# code here\n",
    "trigram=[]\n",
    "for i in range(len(unigram)):\n",
    "    if i+2<len(unigram):\n",
    "        trigram.append(unigram[i]+\" \"+unigram[i+1]+\" \"+unigram[i+2])\n",
    "trigram_set = set(trigram)\n",
    "trigram_set = list(trigram_set)\n",
    "for i in range(10):\n",
    "    print(trigram_set[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the probabilities for each unique trigram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "tri_dict={}\n",
    "for i in trigram_set:\n",
    "    tri_dict[i]=(trigram.count(i))/len(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - Generating Text\n",
    "Generate a paragraph with ten sentences each containing 9-15 words (pick the length of the sentence randomly within this range) using you language model. Start with trigrams, use back-off technique (i.e. use n-1 gram) if a token is not available. \n",
    "\n",
    "For each word prediction, get top 5 most probabale words using the n-gram model and then pick the next word randomly from within these. This is being done to avoid excessive repetitive sequences in your generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  (10, 'کہاجی کہنے سوال جواب پرنسپل مشورہ رضا مند چنانچہ بی اے سرٹفکیٹ جانا')\n",
      "Length:  (9, 'مکھیاں مچھر مارنے کئی کئی افسر مقرر اگلے سال پیشین گوئی ضرورت')\n",
      "Length:  (13, 'قطع روزگارڈن کہلاتا عظیم الشان تصانیف خدا عظمت آثار دکھائی صبح وقت الله میاں یاد چیز')\n",
      "Length:  (12, 'کرکٹ ٹیم ڈنر شامل مطمح نظروسیع تقریر عام تینوں موقعوں کام آسکتی چنانچہ سامعین سہولت')\n",
      "Length:  (10, 'کیجئے مصنوعی پن دیجئے بدولت غریب نام مانوس گے دوپہر وقت درختوں سایے')\n",
      "Length:  (10, 'الجھن  معلوم پرچوں لکھ اچھی جانتا ممتحن لوگ نشے حالت پرچے دیکھیں')\n",
      "Length:  (12, 'طریقہ باقی فارسی فیل گے اگلے سال مطلب یوں ادا ہاسٹل آب وہوا اچھی صفائی')\n",
      "Length:  (14, 'عورتوں کائنات رہبر مردوں حشرات الارض سمجھتی بات نظرانداز میبل دن دس بارہ شادیاں بیٹھتے چنانچے گھر')\n",
      "Length:  (14, 'رساند دانا اندراں حیراں بماند ڈیڑھ مہینے شخصیت ہاسٹل زندگی انحصا ر دومضمونوں وقتا فوقتا خیالات اظہار')\n",
      "Length:  (12, 'نسخ صناعی ہنرمندی لکھے کاتب قدرت آفرین ہوگا پنجاب خطہ خطہ نستعلیق جدید جمیل طرز')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "length_rand = [9,10,11,12,13,14,15]\n",
    "\n",
    "\n",
    "def prob(x):\n",
    "    y = pd.DataFrame.from_dict(x, orient='index', columns=['prob'])\n",
    "    y = y.sort_values(by=['prob'], ascending=False)\n",
    "    y = y[0:5]\n",
    "    y = y.to_dict()\n",
    "    y = random.choice(list(y['prob'].keys()))\n",
    "    return y\n",
    "\n",
    "#select first word\n",
    "\n",
    "def text():\n",
    "    x = random.choice(list(uni_dict.keys()))\n",
    "    #select second word\n",
    "    second = {}\n",
    "    for i in bi_dict:\n",
    "        if i.startswith(x):\n",
    "            second[i] = bi_dict[i]\n",
    "    sec = prob(second)\n",
    "    #add second word to sentence\n",
    "    sentence = x+\" \"+sec.split(\" \")[1]\n",
    "\n",
    "    #select third word\n",
    "    third = {}\n",
    "    for i in tri_dict:\n",
    "        if i.startswith(sec):\n",
    "            third[i] = tri_dict[i]\n",
    "    third = prob(third)\n",
    "    #add third word to sentence\n",
    "    sentence = sentence+\" \"+third.split(\" \")[2]\n",
    "    length = random.choice(length_rand)\n",
    "    for i in range(length):\n",
    "        #select ith word\n",
    "        temp = sentence.split(\" \")\n",
    "        temp = temp[-2]+\" \"+temp[-1]\n",
    "        #print(temp)\n",
    "        third = {}\n",
    "        for i in tri_dict:\n",
    "            if i.startswith(temp):\n",
    "                third[i] = tri_dict[i]\n",
    "\n",
    "        if third == {}:\n",
    "            temp = temp.split(\" \")\n",
    "            temp = temp[1]\n",
    "            third = {}\n",
    "            for i in bi_dict:\n",
    "                if i.startswith(temp):\n",
    "                    third[i] = bi_dict[i]\n",
    "            if third == {}:\n",
    "                temp = random.choice(list(uni_dict.keys()))\n",
    "                sentence = sentence + \" \" + temp\n",
    "                continue\n",
    "\n",
    "            third = prob(third)\n",
    "            sentence += \" \"+third.split(\" \")[1]\n",
    "            continue\n",
    "        third = prob(third)\n",
    "        #add ith word to sentence\n",
    "        sentence += \" \"+third.split(\" \")[2]\n",
    "    return length,sentence\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"Length: \",text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 - Discussion and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Analyze the text generated, and mention 3 distinct observations. Also compare it with the input text and how different it is and why might that be.\n",
    "- Is going upto n=3 enough? What do you think would be a good value of n and why?\n",
    "\n",
    "Answer here:\n",
    "1. The sentence does not finish naturally, instead it tends to finish abruptly. The sentences are missing connective words like tum aur etc because we removed the stop words. The sentences start abrupty as well. This is because our model does not consider where the sentences in the input data start or end. Maybe we can assign weights to identify these words.\n",
    "\n",
    "2. having a high n value will be problematic as the rarity of n grams will increase, the model will take higher time to search the data for such a specific n gram and it wont most likely exist. A very low value will result in very inaccurate predictions. A good value would either be 3 or 4. We should test the model at n=4 as well to see if we get more accurate anwsers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c3d4d9970fc5c09c50d9318a9c7fbf4d0159e4b28833276f5678ee22bded273"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
